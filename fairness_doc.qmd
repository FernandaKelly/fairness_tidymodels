---
title: "Fairness" # Título do relatório
subtitle: "**Fairness com Tidymodels**"
author: "Fernanda Kelly R. Silva | www.fernandakellyrs.com"
lang: pt 
date: "`r format(Sys.Date())`" 
date-format: short 
toc: true 
format: 
    html: 
      embed-resources: true
      #css: ["custom.css"] 
      code-fold: false 
      code-tools: true  
      theme: 
        light: cosmo
        dark: superhero 
#title-block-banner: "#874a9c" 
code-annotations: hover 
execute:
  warning: false
  message: false
  echo: true
---

# Universo Tidymodels

Os pacotes principais do **tidymodels** trabalham juntos para permitir uma ampla variedade de abordagens de modelagem e eles são:

-   **rsample** que fornece infraestrutura para divisão e reamostragem eficiente de dados;

-   **parnsip** é uma interface organizada e unificada para modelos que pode ser usada para testar uma variedade de modelos sem se prender às minúcias sintáticas dos pacotes subjacentes;

-   **recipes** é uma interface organizada para ferramentas de pré-processamento de dados para engenharia de recursos;

-   **workflows** (fluxos de trabalho) agrupam pré-processamento, modelagem e pós-processamento;

-   **tune** ajuda a otimizar os hiperparâmetros do seu modelo e as etapas de pré-processamento;

-   **yardstick** (critério) mede a eficácia dos modelos usando métricas de desempenho;

-   **broom** converte as informações em objetos R estatísticos comuns em formatos previsíveis e fáceis de usar;

-   **dials** cria e gerencia parâmetros de ajuste e grades de parâmetros.

A estrutura tidymodels também inclui muitos outros pacotes projetados para análise de dados especializada e tarefas de modelagem. Eles não são carregados automaticamente com library(tidymodels), então você precisará carregar cada um com sua própria chamada para library().

# Pacote: yardstick

A versão 1.3.0 do Yardstick introduziu uma implementação para **métricas de grupo**. O caso de uso que motiva a implementação desta funcionalidade são as métricas de justiça, embora as métricas de grupo tenham aplicações além desse domínio. As métricas de justiça quantificam o grau de disparidade em um valor de métrica entre grupos.

A título de exemplo vamos utilizar o conjunto de dados **hpc_cv**, contendo probabilidades de classe e previsões de classe para uma análise discriminante linear ajustada ao conjunto de dados HPC de Kuhn e Johnson (2013).

```{r}
library(yardstick)
library(dplyr)
```

## Dados

Esses dados são os conjuntos de avaliação de um esquema de validação cruzada de 10 vezes. Este quadro de dados possui as variáveis de **classe verdadeira** (obs), a **previsão da classe** (pred) e as **colunas para cada probabilidade de classe** (colunas VF, F, M e L). Além disso, é incluída uma coluna para o **indicador de reamostragem** (resample).

```{r}
dados_exemplo1 <- tibble::tibble(yardstick::hpc_cv)
head(dados_exemplo1, 5)
```

Para os propósitos do exemplo, também adicionaremos uma coluna denominada por **batch** aos dados e selecionaremos as colunas para as probabilidades de classe, das quais não precisamos.

```{r}
set.seed(1)

hpc <- dados_exemplo1 %>% 
       dplyr::mutate(batch = base::sample(c("a", "b"), nrow(.), replace = TRUE)) %>% 
       select(-c(VF, F, M, L))
```

## Conscientização de grupo

Mesmo antes da implementação das métricas de grupo, todas as métricas de referência já tinham consciência do grupo. Quando dados agrupados são passados para uma métrica com reconhecimento de grupo, eles retornarão valores de métrica calculados para cada grupo. Logo, se temos 10 folds, e se quiséssemos calcular a precisão do modelo reamostrado, poderíamos escrever:

```{r}
hpc %>% 
  dplyr::group_by(Resample) %>%
  yardstick::accuracy(obs, pred)
```

Veja que aqui nós temos como produto a estimação da acurácia de cada fold ou grupo de interesse. Esse comportamento é o que entendemos por consciência de grupo.

# Métricas de grupo

As métricas de grupo são associadas a uma coluna de dados de modo que, quando os dados são transmitidos a essa coluna, a métrica agrupará temporariamente por essa coluna, calculará valores para cada um dos grupos definidos pela coluna e, em seguida, agregará os valores calculados para o agrupamento temporário de volta ao nível de agrupamento dos dados de entrada.

Suponha que os **batch** nos dados representem dois grupos para os quais o desempenho do modelo não deva diferir. Para quantificar o grau em que o desempenho do modelo difere para esses dois grupos, poderíamos calcular os valores de precisão para cada grupo separadamente e, em seguida, calcular a diferença. Veja o exemplo abaixo.

```{r}
acc_by_group <- hpc %>% 
                dplyr::filter(Resample == "Fold01") %>%
                dplyr::group_by(batch) %>%
                yardstick::accuracy(obs, pred)
acc_by_group
```

Vamos observar a diferença entre os **batch**:

```{r}
base::diff(c(acc_by_group$.estimate[2], acc_by_group$.estimate[1]))
```

As métricas de grupo codificam a função **group_by()**, etapa de agregação amostrada acima em uma métrica de critério. Podemos definir uma nova métrica groupwise com a função **new_groupwise_metric()** do pacote new_groupwise_metric.

Vamos entender os parâmetros dessa função?

-   fn: Uma função métrica de critério ou conjunto de métricas. As métricas disponíveis no pacote são diversas, mas algumas delas são:

    -   detection_prevalence
    -   accuracy
    -   average_precision
    -   classification_cost
    -   poisson_log_loss
    -   precision
    -   roc_auc
    -   recall
    -   rmse
    -   sens
    -   spec

e você consegue ter mais informações sobre as métricas no [CRAN do pacote](https://cran.r-project.org/web/packages/yardstick/index.html).

-   name: O nome da métrica a ser colocada na coluna .metric da saída.

-   aggregate: Uma função para resumir os resultados do conjunto de métricas gerado. A função pega os resultados do conjunto de métricas como o primeiro argumento e retorna um único valor numérico fornecendo o valor **.estimate** como saída.

No exemplo abaixo estamos utilizando a métrica **accuracy**:

```{r}
accuracy_diff <- yardstick::new_groupwise_metric(
                               fn = accuracy,
                               name = "accuracy_diff",
                               aggregate = function(acc_by_group){
                               base::diff(c(acc_by_group$.estimate[2], acc_by_group$.estimate[1]))  
                               }
)
```

Veja que a saída **accuracy_diff** é um objeto da classe **metric_factory**.

```{r}
class(accuracy_diff)
```

A partir de agora a função **accuracy_diff** sabe como obter valores de acurácia para cada grupo e depois retornar a diferença entre a acurácia do primeira e do segundo resultado como saída.

A última coisa que precisamos associar ao objeto é o nome da variável de agrupamento para a qual passar **group_by()**. Podemos passar o nome da variável para **accuracy_diff** fazer isso:

```{r}
accuracy_diff_by_batch <- accuracy_diff(batch)
accuracy_diff_by_batch
```

Logo, podemos usar a a**ccuracy_diff_by_batch()** como métrica da mesma forma que usaríamos **accuracy()**. Veja o exemplo abaixo:

```{r}
hpc %>% 
  dplyr::filter(Resample == "Fold01") %>%
  accuracy_diff_by_batch(obs, pred)
```

Também podemos adicionar **accuracy_diff_by_batch()** aos conjuntos de métricas:

```{r}
acc_ms <- yardstick::metric_set(accuracy, accuracy_diff_by_batch)
acc_ms
```

Aplicando as métricas no **Fold01** da base de dados:

```{r}
hpc %>% 
  filter(Resample == "Fold01") %>%
  acc_ms(truth = obs, estimate = pred)
```

Veja que a saída **.by** nos informa que o agrupamento foi feito através do **batch**, o que nos traz como inferência que as métricas de grupo reconhecem o grupo. Quando essa operação é aplicada a dados com quaisquer variáveis com agrupamentos diferentes da coluna passada como o primeiro argumento para **accuracy_diff()**, neste caso, **accuracy_diff_by_batch()**, as métricas se comportarão como qualquer outra métrica de critério. Por exemplo:

```{r}
hpc %>% 
  group_by(Resample) %>%
  accuracy_diff_by_batch(obs, pred)
```

As métricas de grupo formam o *back-end* das métricas de fairness em modelos organizados e, a partir dos conhecimentos adquidos até o momento, vamos estudá-los.

# Métricas Fairness

::: panel-tabset
# demographic_parity()

A função **demographic_parity** tem o objetivo que avaliar a paridade demográfica e essa é sastifeita quando as previsões de um modelo têm a mesma taxa positiva prevista entre os grupos.

Seu único parâmetro é:

-   by: O identificador de coluna do recurso confidencial. Este deve ser um nome de coluna sem aspas, referindo-se a uma coluna nos dados não pré-processados.

Vamos entender um pouco mais?

Esta função gera uma função métrica de **critério de justiça**. Dada uma variável de agrupamento **by**, a função **demographic_parity()** retornará uma função métrica de critério, como vimos anteriormente, que está associada ao agrupamento de variáveis de dados do parâmetro **by** e a um pós-processador.

A função gerada primeiro gerará um conjunto de valores de métrica de **detection_prevalence** por grupo antes de resumir entre os grupos usando a função de pós-processamento.

**A função gerada possui apenas um método de quadro de dados e deve ser usada como parte de um conjunto de métricas.**

Por padrão, essa função considera a diferença no intervalo da métrica **detection_prevalence** a partir do **.estimate** entre os grupos. O que significa que a disparidade entre pares entre os grupos é o valor de retorno da função.

Veja o exemplo abaixo.

-   (1°) Vamos atualizar o grupo de métricas:

```{r}
acc_ms <- yardstick::metric_set(sens, accuracy, accuracy_diff_by_batch, demographic_parity(Resample))
acc_ms
```

Vamos utilizar a variável **Resample** como indicação de grupos.

-   (2°) Aplicando a base de dados:

```{r}
hpc %>%
  acc_ms(truth = obs, estimate = pred)
```

O que o resultado **.estimate** está nos contando?

| .metric            | **.estimator** | **.estimate** | **.by**  |
|:-------------------|----------------|---------------|----------|
| demographic_parity | macro          | 2.775558e-17  | Resample |


Um valor 0 (ou próximo de 0) indica paridade entre os grupos. Observe que esta definição não depende do verdadeiro resultado. O parâmetro **truth** é incluído nas métricas geradas para fins de consistência.

Esse resultado é esperado, visto que todos os folds estão balanceados de acordo com o número de observações.

```{r}
table(hpc$Resample)
```

# equal_opportunity()

A função **equal_opportunity()** é satisfeita quando as previsões de um modelo têm as mesmas taxas de verdadeiros positivos e falsos negativos em todos os grupos protegidos. 

O cálculo dessa função é baseado na diferença entre o maior e o menor valor da função **sens()** entre grupos, em que **sens()** calcula a sensibilidade de uma mensuração para um resultado de referência (o parâmetro "truth" ou algum padrão ouro).

O parâmetro da função é idêntico ao da função **demographic_parity()**. 

Vamos ao exemplo?

Assim como no exemplo anterior, precisamos acrescentar a métrica no vetor de métricas que desejamos calcular..

```{r}
acc_ms <- yardstick::metric_set(sens, accuracy, accuracy_diff_by_batch,
                                demographic_parity(Resample), equal_opportunity(Resample))
acc_ms
```

Um valor 0 indica paridade entre os grupos e o nosso resultado indica que há igualdade de oportunidades.

```{r}
hpc %>%
  acc_ms(truth = obs, estimate = pred)
```

# equalized_odds()

As probabilidades equalizadas são satisfeitas quando as previsões de um modelo têm as mesmas taxas de falsos positivos, verdadeiros positivos, falsos negativos e verdadeiros negativos em todos os grupos protegidos.


Assim como no exemplo anterior, precisamos acrescentar a métrica no vetor de métricas que desejamos calcular..

```{r}
acc_ms <- yardstick::metric_set(sens, accuracy, accuracy_diff_by_batch,
                                demographic_parity(Resample), equal_opportunity(Resample), equalized_odds(Resample))
acc_ms
```

Um valor 0 indica paridade entre os grupos e o nosso resultado indica que há probabilidades equalizadas.

```{r}
hpc %>%
  acc_ms(truth = obs, estimate = pred)
```
:::

# Exemplos reais

## Are GPT detectors fair? A machine learning fairness case study

Os lançamentos de modelos de linguagem são frequentemente seguidos por detectores, que afirmam detectar se o texto foi escrito por um ser humano ou por um determinado modelo de linguagem. Um estudo de 2023 argumenta que os detectores GPT classificam desproporcionalmente a escrita real de escritores ingleses não nativos como gerada por IA.

## Fair prediction of hospital readmission: a machine learning fairness case study

Com informações sobre a permanência de um paciente com diabetes em um hospital, como dados demográficos, resultados de diagnóstico, pagamento e medicamentos, um hospital pode treinar um modelo de aprendizado de máquina para prever razoavelmente bem se um paciente será readmitido dentro de 30 dias. 


Mas que danos para os pacientes poderiam resultar do uso de tal modelo?


Em 2019, Obermeyer et al. (2019) publicaram uma análise das previsões de um modelo de aprendizado de máquina que os prestadores de cuidados de saúde usam para alocar recursos. Os resultados do modelo são usados ​​para recomendar um paciente para programas de gestão de cuidados de alto risco.

Estes programas procuram melhorar o atendimento a pacientes com necessidades de saúde complexas, fornecendo recursos adicionais, incluindo maior atenção de prestadores treinados, para ajudar a garantir que os cuidados sejam bem coordenados. A maioria dos sistemas de saúde utiliza estes programas como a pedra angular dos esforços de gestão da saúde da população, e são amplamente considerados eficazes na melhoria dos resultados e da satisfação, ao mesmo tempo que reduzem os custos. […] Como os próprios programas são caros — com custos destinados a equipas de enfermeiros dedicados, horários extra de consultas de cuidados primários e outros recursos escassos — **os sistemas de saúde dependem extensivamente de algoritmos para identificar os pacientes que irão beneficiar mais**.

Eles argumentam em sua análise que o modelo em questão apresenta um viés racial substancial, onde “os pacientes negros aos quais foi atribuído o mesmo nível de risco pelo algoritmo são mais doentes do que os pacientes brancos”. Na prática, isto resultou na redução “do número de pacientes negros identificados para cuidados adicionais em mais de metade”.


Este artigo demonstrará um fluxo de trabalho orientado para a justiça para treinar um modelo de aprendizado de máquina para identificar pacientes de alto risco. Ao longo do processo de desenvolvimento do modelo, consideraremos os impactos sociais que diferentes decisões de modelagem podem ter quando tal modelo é implantado no contexto.

## Dados

Os dados que usaremos nesta análise são um banco de dados disponível publicamente que contém informações sobre 71.515 internações hospitalares de pacientes com diabetes. Os dados vêm de um estudo de Strack et al. (2014), onde os autores modelam a eficácia de um determinado teste laboratorial na previsão de readmissão.

Uma versão desses dados está disponível no pacote R de **readmission**:

```{r}
#install.packages("readmission")
library(readmission)
```

O banco de dados **readmission** possui 12 variáveis, como pode ser visto abaixo.

```{r}
dataFairness <- readmission::readmission
utils::head(dataFairness,5)
```

Essas variáveis são:

- readmitted: Se o paciente foi readmitido nos 30 dias seguintes à alta. Um fator com níveis “Sim” e “Não”.

- race: Raça relatada do paciente. Os dados de origem não documentam a estratégia de recolha de dados. Um fator com níveis "Afro-americano", "Asiático", "Caucasiano", "Hispânico", "Outro" e "Desconhecido".

- sex: Sexo relatado do paciente. Os dados de origem não documentam a estratégia de recolha de dados. Um fator com níveis “Feminino” e “Masculino”.

- age: Faixa etária do paciente, agrupada em intervalos de 10 anos. Um fator com os seguintes níveis:

    - [0-10)
    - [10-20)
    - [20-30)
    - [30-40)
    - [40-50)
    - [50-60)
    - [60-70)
    - [70-80)
    - [80-90)
    - [90-100)

- admission_source: Se o paciente foi encaminhado por um médico, admitido pelo pronto-socorro ou chegou por alguma outra fonte. Fator com níveis “Emergência”, “Outros” e “Encaminhamento”.

- blood_glucose: Resultados de um teste A1C, estimando a média de açúcar no sangue do paciente nos últimos 2-3 meses. Níveis médios estimados de glicose no sangue mais elevados estão associados a complicações do diabetes. Um fator com níveis "Normal", "Alto" e "Muito Alto" e muitos valores ausentes.

- insurer: A seguradora de saúde (ou a falta dela, via “Autopagamento”) do paciente. Um fator com níveis "Medicaid", "Medicare", "Privado" e "Self-Pay" e muitos valores ausentes.

- duration: Número de dias de internação entre a admissão e a alta.

- n_previous_visits: Número de consultas de emergência, internação e ambulatorial no ano anterior ao encontro.

- n_diagnoses: “Número de diagnósticos inseridos no sistema” durante o encontro.

- n_procedures: “Número de procedimentos (exceto testes de laboratório) realizados” durante o encontro.

- n_medications: “Número de nomes genéricos distintos administrados” durante o encontro.


## Análise Descritiva

Observe abaixo as porcentagens onde houve readmissão por raça. Veja que **9%** dos Caucasian e **8.50%** dos African American tiveram uma readmissão, mas entre aqueles que foram readmitidos em até 30 dias após a alta hospitalar Caucasian tiveram **76.56%** e African American **17.37%** de readmissão.


```{r}
library(janitor)

dataFairness %>% 
   dplyr::group_by(race, readmitted) %>% 
   dplyr::count() 
   
```

Podemos constatar que pessoas consideradas **Caucasian** e **African American** são quase **93%** de toda a população da base de dados e que **75%** é composto por apenas por pessoas **Caucasian**. As outras raças **Asian**, **Hispanic**, **Other** e **Unknown** possuem frequência baixa e, consequentemente, discrepantes das categorias de interesse. Essa discrepância **também** é um fator que deve ser considerado nas análises.

```{r}
dataFairness %>% 
   janitor::tabyl(race, show_na = TRUE) %>% 
   dplyr::mutate(porcent_n = round(percent*100,2))
```

Nós temos três opções para lidar com essa discrepância, visto que na reamostragem, a análise das categorias de baixa frequência podem interferir nos resultados.

```{r}
library(tidyverse)
dataFairness %>%
  dplyr::mutate(., group = sample(1:10, nrow(.), replace = TRUE)) %>%
  dplyr::group_by(race, group) %>%
  dplyr::summarize(prop = mean(readmitted == "Yes"), n = n()) %>%
  dplyr::summarize(mean = mean(prop), sd = sd(prop), n = sum(n))

```


As opções:

-   Remova linhas decorrentes de classes pouco frequentes. Neste exemplo, isso significaria remover todas as linhas com valores diferentes de "Caucasian" ou "African American". Esta é a abordagem adotada por outras análises públicas destes dados, incluindo o estudo original do qual estes dados surgiram. Uma análise resultante desta abordagem ignoraria quaisquer disparidades no cuidado de grupos diferentes de "Caucasian" ou "African American".
-   Crie métricas de justiça personalizadas que abordem o aumento da variabilidade associada a contagens menores. Poderíamos dimensionar a variabilidade das estimativas para cada grupo antes de calcular as métricas de justiça.
-   Agrupar os valores das categorias pouco frequentes. Isso implicaria o colapso de valores dos grupos diferentes de "Caucasian"e "African American" para um nível de fator. Esta abordagem é um tanto híbrida das duas abordagens acima; perdemos alguma granularidade nas informações relativas ao atendimento dos grupos diferentes de "Caucasian"e "African American", mas reduzimos a variabilidade associada às estimativas para esses grupos no processo.

**Em nosso caso, vamos considerar a última opção.**

Observe abaixo as porcentagens onde houve readmissão por raça. Veja que **9%** dos caucasianos e **8.50%** dos afro-americados tiveram uma readmissão, mas entre aqueles que foram readmitidos em até 30 dias após a alta hospitalar caucasianos tiveram **76.56%** e afro-americanos **17.37%** de readmissão.


```{r}
library(janitor)

dataFairness %>% 
   dplyr::group_by(race, readmitted) %>% 
   dplyr::count() 
   
```

Atualização do projeto mais uma vez.







