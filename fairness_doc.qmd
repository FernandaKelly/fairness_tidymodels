---
title: "Fairness" # Título do relatório
subtitle: "**Fairness com Tidymodels**"
author: "Fernanda Kelly R. Silva | www.fernandakellyrs.com"
lang: pt 
date: "`r format(Sys.Date())`" 
date-format: short 
toc: true 
format: 
    html: 
      embed-resources: true
      #css: ["custom.css"] 
      code-fold: false 
      code-tools: true  
      theme: 
        light: cosmo
        dark: superhero 
#title-block-banner: "#874a9c" 
code-annotations: hover 
execute:
  warning: false
  message: false
  echo: true
---

# Universo Tidymodels

Os pacotes principais do **tidymodels** trabalham juntos para permitir uma ampla variedade de abordagens de modelagem e eles são:

-   **rsample** que fornece infraestrutura para divisão e reamostragem eficiente de dados;

-   **parnsip** é uma interface organizada e unificada para modelos que pode ser usada para testar uma variedade de modelos sem se prender às minúcias sintáticas dos pacotes subjacentes;

-   **recipes** é uma interface organizada para ferramentas de pré-processamento de dados para engenharia de recursos;

-   **workflows** (fluxos de trabalho) agrupam pré-processamento, modelagem e pós-processamento;

-   **tune** ajuda a otimizar os hiperparâmetros do seu modelo e as etapas de pré-processamento;

-   **yardstick** (critério) mede a eficácia dos modelos usando métricas de desempenho;

-   **broom** converte as informações em objetos R estatísticos comuns em formatos previsíveis e fáceis de usar;

-   **dials** cria e gerencia parâmetros de ajuste e grades de parâmetros.

A estrutura tidymodels também inclui muitos outros pacotes projetados para análise de dados especializada e tarefas de modelagem. Eles não são carregados automaticamente com library(tidymodels), então você precisará carregar cada um com sua própria chamada para library().

# Pacote: yardstick

A versão 1.3.0 do Yardstick introduziu uma implementação para **métricas de grupo**. O caso de uso que motiva a implementação desta funcionalidade são as métricas de justiça, embora as métricas de grupo tenham aplicações além desse domínio. As métricas de justiça quantificam o grau de disparidade em um valor de métrica entre grupos.

A título de exemplo vamos utilizar o conjunto de dados **hpc_cv**, contendo probabilidades de classe e previsões de classe para uma análise discriminante linear ajustada ao conjunto de dados HPC de Kuhn e Johnson (2013).

```{r}
library(yardstick)
library(dplyr)
```

## Dados

Esses dados são os conjuntos de avaliação de um esquema de validação cruzada de 10 vezes. Este quadro de dados possui as variáveis de **classe verdadeira** (obs), a **previsão da classe** (pred) e as **colunas para cada probabilidade de classe** (colunas VF, F, M e L). Além disso, é incluída uma coluna para o **indicador de reamostragem** (resample).

```{r}
dados_exemplo1 <- tibble::tibble(yardstick::hpc_cv)
head(dados_exemplo1, 5)
```

Para os propósitos do exemplo, também adicionaremos uma coluna denominada por **batch** aos dados e selecionaremos as colunas para as probabilidades de classe, das quais não precisamos.

```{r}
set.seed(1)

hpc <- dados_exemplo1 %>% 
       dplyr::mutate(batch = base::sample(c("a", "b"), nrow(.), replace = TRUE)) %>% 
       select(-c(VF, F, M, L))
```

## Conscientização de grupo

Mesmo antes da implementação das métricas de grupo, todas as métricas de referência já tinham consciência do grupo. Quando dados agrupados são passados para uma métrica com reconhecimento de grupo, eles retornarão valores de métrica calculados para cada grupo. Logo, se temos 10 folds, e se quiséssemos calcular a precisão do modelo reamostrado, poderíamos escrever:

```{r}
hpc %>% 
  dplyr::group_by(Resample) %>%
  yardstick::accuracy(obs, pred)
```

Veja que aqui nós temos como produto a estimação da acurácia de cada fold ou grupo de interesse. Esse comportamento é o que entendemos por consciência de grupo.

# Métricas de grupo

As métricas de grupo são associadas a uma coluna de dados de modo que, quando os dados são transmitidos a essa coluna, a métrica agrupará temporariamente por essa coluna, calculará valores para cada um dos grupos definidos pela coluna e, em seguida, agregará os valores calculados para o agrupamento temporário de volta ao nível de agrupamento dos dados de entrada.

Suponha que os **batch** nos dados representem dois grupos para os quais o desempenho do modelo não deva diferir. Para quantificar o grau em que o desempenho do modelo difere para esses dois grupos, poderíamos calcular os valores de precisão para cada grupo separadamente e, em seguida, calcular a diferença. Veja o exemplo abaixo.

```{r}
acc_by_group <- hpc %>% 
                dplyr::filter(Resample == "Fold01") %>%
                dplyr::group_by(batch) %>%
                yardstick::accuracy(obs, pred)
acc_by_group
```

Vamos observar a diferença entre os **batch**:

```{r}
base::diff(c(acc_by_group$.estimate[2], acc_by_group$.estimate[1]))
```

As métricas de grupo codificam a função **group_by()**, etapa de agregação amostrada acima em uma métrica de critério. Podemos definir uma nova métrica groupwise com a função **new_groupwise_metric()** do pacote new_groupwise_metric.

Vamos entender os parâmetros dessa função?

-   fn: Uma função métrica de critério ou conjunto de métricas. As métricas disponíveis no pacote são diversas, mas algumas delas são:

    -   detection_prevalence
    -   accuracy
    -   average_precision
    -   classification_cost
    -   poisson_log_loss
    -   precision
    -   roc_auc
    -   recall
    -   rmse
    -   sens
    -   spec

e você consegue ter mais informações sobre as métricas no [CRAN do pacote](https://cran.r-project.org/web/packages/yardstick/index.html).

-   name: O nome da métrica a ser colocada na coluna .metric da saída.

-   aggregate: Uma função para resumir os resultados do conjunto de métricas gerado. A função pega os resultados do conjunto de métricas como o primeiro argumento e retorna um único valor numérico fornecendo o valor **.estimate** como saída.

No exemplo abaixo estamos utilizando a métrica **accuracy**:

```{r}
accuracy_diff <- yardstick::new_groupwise_metric(
                               fn = accuracy,
                               name = "accuracy_diff",
                               aggregate = function(acc_by_group){
                               base::diff(c(acc_by_group$.estimate[2], acc_by_group$.estimate[1]))  
                               }
)
```

Veja que a saída **accuracy_diff** é um objeto da classe **metric_factory**.

```{r}
class(accuracy_diff)
```

A partir de agora a função **accuracy_diff** sabe como obter valores de acurácia para cada grupo e depois retornar a diferença entre a acurácia do primeira e do segundo resultado como saída.

A última coisa que precisamos associar ao objeto é o nome da variável de agrupamento para a qual passar **group_by()**. Podemos passar o nome da variável para **accuracy_diff** fazer isso:

```{r}
accuracy_diff_by_batch <- accuracy_diff(batch)
accuracy_diff_by_batch
```

Logo, podemos usar a a**ccuracy_diff_by_batch()** como métrica da mesma forma que usaríamos **accuracy()**. Veja o exemplo abaixo:

```{r}
hpc %>% 
  dplyr::filter(Resample == "Fold01") %>%
  accuracy_diff_by_batch(obs, pred)
```

Também podemos adicionar **accuracy_diff_by_batch()** aos conjuntos de métricas:

```{r}
acc_ms <- yardstick::metric_set(accuracy, accuracy_diff_by_batch)
acc_ms
```

Aplicando as métricas no **Fold01** da base de dados:

```{r}
hpc %>% 
  filter(Resample == "Fold01") %>%
  acc_ms(truth = obs, estimate = pred)
```

Veja que a saída **.by** nos informa que o agrupamento foi feito através do **batch**, o que nos traz como inferência que as métricas de grupo reconhecem o grupo. Quando essa operação é aplicada a dados com quaisquer variáveis com agrupamentos diferentes da coluna passada como o primeiro argumento para **accuracy_diff()**, neste caso, **accuracy_diff_by_batch()**, as métricas se comportarão como qualquer outra métrica de critério. Por exemplo:

```{r}
hpc %>% 
  group_by(Resample) %>%
  accuracy_diff_by_batch(obs, pred)
```

As métricas de grupo formam o *back-end* das métricas de fairness em modelos organizados e, a partir dos conhecimentos adquidos até o momento, vamos estudá-los.

# Métricas Fairness

::: panel-tabset
# demographic_parity()

A função **demographic_parity** tem o objetivo que avaliar a paridade demográfica e essa é sastifeita quando as previsões de um modelo têm a mesma taxa positiva prevista entre os grupos.

Seu único parâmetro é:

-   by: O identificador de coluna do recurso confidencial. Este deve ser um nome de coluna sem aspas, referindo-se a uma coluna nos dados não pré-processados.

Vamos entender um pouco mais?

Esta função gera uma função métrica de **critério de justiça**. Dada uma variável de agrupamento **by**, a função **demographic_parity()** retornará uma função métrica de critério, como vimos anteriormente, que está associada ao agrupamento de variáveis de dados do parâmetro **by** e a um pós-processador.

A função gerada primeiro gerará um conjunto de valores de métrica de **detection_prevalence** por grupo antes de resumir entre os grupos usando a função de pós-processamento.

**A função gerada possui apenas um método de quadro de dados e deve ser usada como parte de um conjunto de métricas.**

Por padrão, essa função considera a diferença no intervalo da métrica **detection_prevalence** a partir do **.estimate** entre os grupos. O que significa que a disparidade entre pares entre os grupos é o valor de retorno da função.

Veja o exemplo abaixo.

-   (1°) Vamos atualizar o grupo de métricas:

```{r}
acc_ms <- yardstick::metric_set(sens, accuracy, accuracy_diff_by_batch, demographic_parity(Resample))
acc_ms
```

Vamos utilizar a variável **Resample** como indicação de grupos.

-   (2°) Aplicando a base de dados:

```{r}
hpc %>%
  acc_ms(truth = obs, estimate = pred)
```

O que o resultado **.estimate** está nos contando?

| .metric            | **.estimator** | **.estimate** | **.by**  |
|:-------------------|----------------|---------------|----------|
| demographic_parity | macro          | 2.775558e-17  | Resample |


Um valor 0 (ou próximo de 0) indica paridade entre os grupos. Observe que esta definição não depende do verdadeiro resultado. O **truth** argumento é incluído nas métricas geradas para fins de consistência.

# equal_opportunity()

# equalized_odds()
:::
